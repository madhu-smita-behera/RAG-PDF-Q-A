# -*- coding: utf-8 -*-
"""RAG_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wHjqIvr7-hucVfg5EaFZY7EtxID2Cm5Z

## Step 1: Install the required libraries
- LANGCHAIN: an open source framework for building applications based on LLM (Large Language Models)
- FAISS: Facebook AI Similarity Search allows to quickly search for embeddings of multimedia documents that are similar to each other.
- SENTENCE TRANSFORMERS: used to create dense vector representations (embeddings) of sentences, paragraphs, images.
- HUGGINGFACE HUB: a platform for the machine learning community to share and collaborate on models, datasets, and applications.
- GRADIO: used to create interactive web interfaces for machine learning models, APIs, and any python functions.
- PYPDF: a free and open source python pdf library capable of splitting, merging, cropping, and transforming PDF files.
"""

# app.py

import os
import gradio as gr
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA

# Load Hugging Face Token from environment
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# 1. Load and split the PDF
loader = PyPDFLoader("yourfile.pdf")  # <-- Change to your actual PDF file name
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# 2. Embedding and Vector Store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)

# 3. LLM
llm = HuggingFaceHub(
    repo_id="google/flan-t5-base",
    model_kwargs={"temperature": 0.0, "max_length": 512},
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN
)

# 4. Build RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5. Define Gradio interface
def answer_question(query):
    response = qa_chain.run(query)
    return response

iface = gr.Interface(
    fn=answer_question,
    inputs=gr.Textbox(lines=2, placeholder="Ask a question..."),
    outputs="text",
    title="ðŸ“„ PDF Question Answering",
    description="Ask any question from the uploaded PDF!"
)

iface.launch()

